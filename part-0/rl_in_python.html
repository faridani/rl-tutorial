<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reinforcement Learning with Python – A Hands‑On Guide</title>
    <style>
        body {
            background: #fff;
            color: #222;
            font-family: "Georgia", "Times New Roman", serif;
            line-height: 1.6;
            margin: 0;
            padding: 0 1.5rem;
        }
        h1, h2, h3, h4 {
            color: #003366;
            margin-top: 1.2em;
        }
        pre {
            background: #f8f8f8;
            border: 1px solid #e0e0e0;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            font-family: "Courier New", Courier, monospace;
            color: #C7254E;
        }
        .citation {
            font-size: 0.85em;
            vertical-align: super;
        }
        footer {
            margin-top: 3em;
            border-top: 1px solid #ddd;
            padding: 1em 0;
            font-size: 0.8em;
            color: #555;
        }
        nav {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            background: #fafafa;
            border-bottom: 1px solid #eee;
            padding: 0.5em 1em;
            z-index: 100;
        }
        nav a {
            margin-right: 1em;
            color: #003366;
            text-decoration: none;
        }
        nav a:hover {
            text-decoration: underline;
        }
        section {
            margin-top: 4.5rem;
        }
    </style><script>
  MathJax = {
    tex: {
      inlineMath: [['\\(', '\\)']],
      displayMath: [['\\[', '\\]']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<nav>
    <a href="#intro">Introduction</a>
    <a href="#math">Math in Python</a>
    <a href="#mdp">Markov Decision Processes</a>
    <a href="#pomdp">POMDPs</a>
    <a href="#qlearning">Q‑Learning</a>
    <a href="#dqn">Deep Q‑Networks</a>
    <a href="#policy-gradient">Policy Gradients &amp; PPO</a>
    <a href="#grpo">Group Relative Policy Optimisation</a>
    <a href="#optimisation">Gradient Descent &amp; Adam</a>
    <a href="#conclusion">Conclusion</a>
</nav>

<section id="intro">
    <h1>Reinforcement Learning with Python – A Hands‑On Guide</h1>
    <p>
        Reinforcement learning (RL) is a branch of machine learning concerned with training agents to make
        sequences of decisions that maximise cumulative reward. This guide introduces RL from first
        principles, then gradually builds up to advanced algorithms. Rather than focusing on robotics,
        the examples centre on toy problems such as pricing and decision making.
    </p>
    <p>
        Each section presents the relevant theory with mathematical intuition and cites trusted sources
        from recent literature. Practical code examples demonstrate how to implement the concepts in
        <code>Python</code>, giving you hands‑on experience.
    </p>
</section>

<section id="math">
    <h2>Doing Math in Python</h2>
    <p>
        RL algorithms rely heavily on numerical computation. Python’s built‑in arithmetic and
        NumPy’s vectorised operations allow us to work efficiently with scalars and arrays. Basic
        operations like addition, subtraction, multiplication and division can be performed using
        operators or their corresponding NumPy functions. Programiz lists these functions—
        <code>add</code>, <code>subtract</code>, <code>multiply</code>, <code>divide</code>, <code>power</code> and <code>mod</code>—as
        alternatives to <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>**</code> and <code>%</code>【115198363139400†L353-L364】.
    </p>
    <p>
        The following snippet illustrates element‑wise operations on NumPy arrays. Both the operators
        and the functions produce identical results:
    </p>
    <pre><code class="language-python">import numpy as np

# two arrays representing sales in consecutive days for two products
sales_a = np.array([100, 120, 130, 110])
sales_b = np.array([90, 95, 100, 105])

# element‑wise addition (total sales for each day)
total_sales_op = sales_a + sales_b
# or using the built‑in function
total_sales_fn = np.add(sales_a, sales_b)

# element‑wise subtraction (difference in sales)
diff_sales_op = sales_a - sales_b
diff_sales_fn = np.subtract(sales_a, sales_b)

# element‑wise multiplication (e.g. combine scores and weights)
multiplied_op = sales_a * sales_b
multiplied_fn = np.multiply(sales_a, sales_b)

# element‑wise division (ratio of product A to B sales)
divided_op = sales_a / sales_b
divided_fn = np.divide(sales_a, sales_b)

print("Total sales (operator):", total_sales_op)
print("Total sales (function):", total_sales_fn)
print("Difference (operator):", diff_sales_op)
print("Difference (function):", diff_sales_fn)
print("Product (operator):", multiplied_op)
print("Product (function):", multiplied_fn)
print("Ratio (operator):", divided_op)
print("Ratio (function):", divided_fn)</code></pre>
    <p>
        Running this code prints identical outputs for the operator and function versions, verifying
        that NumPy’s functions mirror the arithmetic operators. Working with arrays is foundational
        when representing state–action values, policies or neural network parameters.
    </p>
</section>

<section id="mdp">
    <h2>Markov Decision Processes</h2>
    <p>
        A <strong>Markov Decision Process (MDP)</strong> is a mathematical framework for sequential decision
        making. According to lecture notes from the University of Alberta, an MDP consists of
        <em>states</em>, <em>actions</em>, stochastic transition rules, rewards and an objective
        function【320210096800443†L118-L136】. A state <code>s</code> belongs to a set \(\mathcal{S}\) and an action <code>a</code>
        belongs to a set \(\mathcal{A}\). The environment’s dynamics are modelled by the transition
        probabilities \(P_a(s,s')\), which specify the probability of landing in state <code>s'</code> after
        taking action <code>a</code> in state <code>s</code>【320210096800443†L118-L136】. A reward function \(r_a(s)\) assigns
        scalar feedback when action <code>a</code> is taken in state <code>s</code>【320210096800443†L138-L142】. The agent’s goal is
        to choose a policy \(\pi\) that maximises the discounted expected return.
    </p>
    <h3>Toy Problem: Optimal Pricing</h3>
    <p>
        To illustrate MDPs, consider a simple pricing problem. A retailer sells a product with
        two possible prices: $5 and $10. The state corresponds to the current price and an
        estimated demand level. The action space consists of “increase price” and “decrease
        price.” Demands are sampled from a distribution depending on the price, and the reward is
        the revenue (price times demand). We simulate this environment below.
    </p>
    <pre><code class="language-python">import numpy as np

class PricingMDP:
    def __init__(self, prices=(5, 10), demand_means=(100, 60), demand_std=10, discount=0.95):
        self.prices = prices
        self.demand_means = demand_means
        self.demand_std = demand_std
        self.discount = discount
        self.state = 0  # index into prices; 0 for low price, 1 for high price

    def reset(self):
        self.state = 0
        return self.state

    def step(self, action):
        """
        action 0: lower price (move to state 0)
        action 1: raise price (move to state 1)
        """
        # transition dynamics: new price index equals action
        self.state = action
        price = self.prices[self.state]
        # simulate random demand around the mean for this price
        demand = max(0, np.random.normal(self.demand_means[self.state], self.demand_std))
        reward = price * demand
        # here there are only two states and both actions are always allowed
        return self.state, reward

# example usage
mdp = PricingMDP()
s = mdp.reset()
for t in range(5):
    # naive policy: alternate between low and high price
    action = t % 2
    next_state, reward = mdp.step(action)
    print(f"Time {t}: action={action}, price={mdp.prices[next_state]}, demand~{reward/mdp.prices[next_state]:.1f}, revenue={reward:.2f}")
</code></pre>
    <p>
        This toy environment is intentionally simple but captures the core components of an MDP: a
        finite state and action space, stochastic transitions (demand fluctuations) and rewards
        tied to actions. In later sections we will apply Q‑learning and policy gradient methods
        to learn optimal pricing policies.
    </p>
</section>

<section id="pomdp">
    <h2>Partially Observable MDPs (POMDPs)</h2>
    <p>
        In many real‑world settings the agent cannot observe the true state directly. A
        <strong>POMDP</strong> extends an MDP by introducing observations and an observation model. The
        components of a POMDP are:
    </p>
    <ul>
        <li>States \(S\) and actions \(A\), just like in an MDP.</li>
        <li>A transition function \(T(s,a,s') = P(s'\mid s,a)\) specifying the probability of the next state【467555468819639†L124-L133】.</li>
        <li>An observation set \(O\) and observation model \(Z(s',a,o) = P(o\mid s',a)\) giving the likelihood of
            observing <code>o</code> after taking action <code>a</code> and landing in state <code>s'</code>【467555468819639†L123-L133】.</li>
        <li>A reward function \(R(s,a)\) and a discount factor \(\gamma\)【467555468819639†L123-L136】.</li>
    </ul>
    <p>
        Because the agent sees only observations, it must maintain a <em>belief state</em>, a
        probability distribution over possible states. Belief updates combine the previous belief,
        the observation model and Bayes’ rule【467555468819639†L140-L151】. In practice, solving POMDPs
        exactly is expensive. Approximate methods (e.g. belief‑state MDPs or Monte Carlo
        sampling) are often used.
    </p>
    <h3>Example: Inventory Management with Noisy Demand Signals</h3>
    <p>
        Imagine a warehouse that cannot directly measure its inventory level because of noisy
        sensors. Each day the agent decides how many units to order. The true inventory level is
        the hidden state; the sensor provides an observation drawn from a normal distribution
        centred on the true level. The reward depends on sales revenue minus holding and
        ordering costs. This is a classic POMDP: actions affect the hidden state, but the agent
        only has noisy observations to guide its decisions. Implementing such an environment
        requires tracking belief states; for brevity the code is omitted, but this example
        motivates advanced RL algorithms able to deal with partial observability.
    </p>
</section>

<section id="qlearning">
    <h2>Q‑Learning</h2>
    <p>
        Q‑learning is a model‑free, value‑based reinforcement learning algorithm. It learns the
        quality (expected return) of taking an action <code>a</code> in state <code>s</code>—the
        Q‑value—through trial and error. GeeksforGeeks explains that the agent interacts with the
        environment and updates a Q‑table using the temporal‑difference update rule【961484668398399†L82-L86】.
    </p>
    <p>
        The core update formula is:
    </p>
    <pre><code>Q(s,a) ← Q(s,a) + α \big(R + γ \max_{a'} Q(s',a') - Q(s,a)\big)</code></pre>
    <p>
        where \(\alpha\) is the learning rate, \(\gamma\) is the discount factor and <code>R</code> is the
        reward【961484668398399†L115-L129】. To balance exploration and exploitation, Q‑learning often
        uses an \(\varepsilon\)-greedy policy: with probability \(1-\varepsilon\) it selects the
        action with the highest Q‑value, and with probability \(\varepsilon\) it picks a random
        action【961484668398399†L131-L140】.
    </p>
    <h3>Pricing Example Revisited</h3>
    <p>
        Let us apply Q‑learning to the pricing MDP defined earlier. The agent learns whether to set
        a low or high price to maximise expected revenue. We will maintain a Q‑table of shape
        <code>(2, 2)</code> (two states × two actions) and update it as the agent interacts with the
        environment.
    </p>
    <pre><code class="language-python">import numpy as np

prices = [5, 10]
# true mean demands used to simulate environment
true_means = [100, 60]

# Q-table initialised to zeros
Q = np.zeros((2, 2))

alpha = 0.1        # learning rate
gamma = 0.95       # discount factor
epsilon = 0.2      # initial exploration probability
num_episodes = 500

for episode in range(num_episodes):
    state = 0  # start from low price
    for t in range(20):
        # epsilon-greedy action selection
        if np.random.rand() &lt; epsilon:
            action = np.random.randint(2)
        else:
            action = np.argmax(Q[state])

        # simulate transition
        price = prices[action]
        demand = max(0, np.random.normal(true_means[action], 10))
        reward = price * demand
        next_state = action  # new state corresponds to chosen price

        # update rule
        best_next = np.max(Q[next_state])
        Q[state, action] += alpha * (reward + gamma * best_next - Q[state, action])

        state = next_state

    # gradually reduce exploration
    epsilon = max(0.01, epsilon * 0.995)

print("Learned Q-table:\n", Q)
# optimal policy: choose action with highest Q-value in each state
policy = np.argmax(Q, axis=1)
print("Optimal pricing policy (0=low price, 1=high price):", policy)
</code></pre>
    <p>
        After training, the Q‑table approximates the expected revenue for each price choice. The
        agent discovers which price maximises revenue given stochastic demand. This illustrates
        how Q‑learning can solve pricing and decision‑making problems without requiring a model of
        the environment.
    </p>
</section>

<section id="dqn">
    <h2>Deep Q‑Networks (DQN)</h2>
    <p>
        When state or action spaces are large or continuous, tabular methods become impractical.
        Deep Q‑learning uses neural networks to approximate the action‑value function. Built In
        describes deep Q‑learning as an extension of Q‑learning where a deep neural network
        approximates Q‑values for each action【622268142678754†L33-L56】. This allows RL agents to
        handle high‑dimensional inputs (e.g. images) and continuous state spaces.
    </p>
    <h3>Implementing DQN for CartPole</h3>
    <p>
        We now implement a deep Q‑network to solve the classic <code>CartPole</code> problem (a
        non‑robotic control task in which a pole is balanced on a cart). We use the
        PyTorch library to build a simple feed‑forward network. The environment comes from
        <code>gymnasium</code> (the successor of OpenAI Gym).
    </p>
    <pre><code class="language-python">import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym

# Hyperparameters
learning_rate = 1e-3
gamma = 0.99
epsilon_start, epsilon_end = 1.0, 0.05
epsilon_decay = 500
batch_size = 64
replay_size = 10000
num_episodes = 300

# Experience replay buffer
def replay_buffer(max_size):
    buffer = []
    def push(sample):
        if len(buffer) &gt;= max_size:
            buffer.pop(0)
        buffer.append(sample)
    def sample(batch_size):
        return random.sample(buffer, batch_size)
    return buffer, push, sample

# Neural network approximating Q(s,a)
class DQN(nn.Module):
    def __init__(self, obs_size, n_actions):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_size, 128), nn.ReLU(),
            nn.Linear(128, 128), nn.ReLU(),
            nn.Linear(128, n_actions)
        )
    def forward(self, x):
        return self.net(x)

# Initialise environment, network and optimiser
env = gym.make('CartPole-v1')
obs_size = env.observation_space.shape[0]
n_actions = env.action_space.n
policy_net = DQN(obs_size, n_actions)
target_net = DQN(obs_size, n_actions)
target_net.load_state_dict(policy_net.state_dict())
optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)
replay, push, sample = replay_buffer(replay_size)

# Epsilon-greedy function
def epsilon_greedy(state, step):
    epsilon = epsilon_end + (epsilon_start - epsilon_end) * \
              np.exp(-1. * step / epsilon_decay)
    if random.random() &lt; epsilon:
        return env.action_space.sample()
    else:
        with torch.no_grad():
            return torch.argmax(policy_net(torch.tensor(state, dtype=torch.float32))).item()

# Training loop
step = 0
for episode in range(num_episodes):
    state, _ = env.reset()
    episode_reward = 0
    done = False
    while not done:
        action = epsilon_greedy(state, step)
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        push((state, action, reward, next_state, done))
        state = next_state
        episode_reward += reward
        step += 1

        # Sample minibatch and update network
        if len(replay) &gt; batch_size:
            batch = sample(batch_size)
            states, actions, rewards, next_states, dones = zip(*batch)
            states = torch.tensor(states, dtype=torch.float32)
            actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)
            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)
            next_states = torch.tensor(next_states, dtype=torch.float32)
            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)

            # Compute Q-values for current states
            q_values = policy_net(states).gather(1, actions)
            # Compute target Q-values using target network
            with torch.no_grad():
                next_q_values = target_net(next_states).max(1, keepdim=True)[0]
                target_q = rewards + gamma * next_q_values * (1 - dones)

            loss = nn.functional.mse_loss(q_values, target_q)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # Periodically update target network
        if step % 100 == 0:
            target_net.load_state_dict(policy_net.state_dict())

    if (episode + 1) % 50 == 0:
        print(f"Episode {episode+1}, Reward: {episode_reward}")

env.close()
</code></pre>
    <p>
        This implementation demonstrates the major DQN components: experience replay, a target
        network to stabilise learning, an ε‑greedy policy for exploration and the temporal‑difference
        loss. After sufficient episodes, the agent learns to balance the pole effectively.
    </p>
</section>

<section id="policy-gradient">
    <h2>Policy Gradient Methods &amp; Proximal Policy Optimisation (PPO)</h2>
    <p>
        Q‑learning and DQN learn value functions. Another family of RL algorithms learns policies
        directly via gradient ascent on expected return. Spinning Up notes that PPO addresses the
        challenge of taking large policy improvement steps without causing performance collapse
        by constraining how far the new policy can deviate from the old policy【724650800826684†L93-L112】.
        PPO is a first‑order method that uses either a KL penalty or a clipping mechanism to
        keep updates within a safe region【724650800826684†L93-L110】.
    </p>
    <p>
        The clipped objective for PPO‑Clip is:
    </p>
    <pre><code>L(s,a,θ_k,θ) = \min\Bigg( \frac{\pi_{θ}(a|s)}{\pi_{\u03b8_k}(a|s)} A^{\pi_{\u03b8_k}}(s,a),
                 \text{clip}\Big(\frac{\pi_{\u03b8}(a|s)}{\pi_{\u03b8_k}(a|s)},1-\epsilon,1+\epsilon\Big)
                 A^{\pi_{\u03b8_k}}(s,a) \Bigg)
    </code></pre>
    <p>
        where \(A^{\pi_{\theta_k}}(s,a)\) is the advantage estimate and \(\epsilon\) is a small
        hyper‑parameter controlling the clip range【724650800826684†L124-L136】. PPO chooses the minimum
        between the unclipped and clipped terms, effectively capping the incentive to move far
        from the old policy.
    </p>
    <h3>Using PPO for a Pricing Task with <code>stable-baselines3</code></h3>
    <p>
        The <code>stable‑baselines3</code> library provides a convenient implementation of PPO. The
        following example wraps the earlier pricing environment in the Gym API and trains a PPO
        agent to learn the optimal pricing policy.
    </p>
    <pre><code class="language-python">!pip install stable-baselines3 gymnasium --quiet

import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env

class PricingEnv(gym.Env):
    """Gym wrapper around the PricingMDP defined earlier."""
    def __init__(self):
        super().__init__()
        self.mdp = PricingMDP()
        # observation space: two discrete states
        self.observation_space = gym.spaces.Discrete(2)
        # two actions: 0 lower price, 1 higher price
        self.action_space = gym.spaces.Discrete(2)
    def reset(self, seed=None, options=None):
        state = self.mdp.reset()
        return state, {}
    def step(self, action):
        next_state, reward = self.mdp.step(int(action))
        # no termination condition; treat every episode as length 10
        terminated = False
        truncated = False
        return next_state, reward, terminated, truncated, {}

# Vectorised environment for parallel training
env = make_vec_env(PricingEnv, n_envs=4)
model = PPO("MlpPolicy", env, verbose=0, gamma=0.95, n_steps=32, ent_coef=0.01)
model.learn(total_timesteps=5000)

# Evaluate trained policy
test_env = PricingEnv()
state, _ = test_env.reset()
for t in range(10):
    action, _states = model.predict(state, deterministic=True)
    next_state, reward, _, _, _ = test_env.step(action)
    print(f"Step {t}: price {test_env.mdp.prices[action]}, reward {reward:.2f}")
    state = next_state
</code></pre>
    <p>
        This example shows how PPO’s clipped objective leads to stable learning. The agent learns a
        pricing strategy without oscillating wildly between high and low prices.
    </p>
</section>

<section id="grpo">
    <h2>Group Relative Policy Optimisation (GRPO)</h2>
    <p>
        Recent research on RL for language models introduced <strong>Group Relative Policy
        Optimisation (GRPO)</strong>, which simplifies policy optimisation by removing the need for a
        learned critic. Instead of estimating the advantage function with a value network as in
        PPO, GRPO normalises rewards within a group of responses. Yuge Shi describes the workflow:
        for each prompt, multiple responses are sampled, a reward model assigns scores, and
        advantages are computed by subtracting the mean and dividing by the standard deviation of
        the group【910949881062373†L382-L403】. This group‑normalised advantage eliminates the critic and
        reduces variance.
    </p>
    <p>
        More formally, given a group of responses \(\mathcal{G} = \{r_1, \dots, r_N\}\), the advantage for
        response \(r_i\) is
    </p>
    <pre><code>\displaystyle A_i = \frac{R_\phi(r_i) - \text{mean}(\mathcal{G})}{\text{std}(\mathcal{G})}</code></pre>
    <p>
        where \(R_\phi\) is a learned reward model【910949881062373†L392-L399】. GRPO still uses the PPO
        surrogate loss with clipping, but the advantage estimates come from group normalisation
        rather than a value function【910949881062373†L426-L441】. In contexts such as RLHF (reinforcement
        learning from human feedback) for large language models, this approach reduces memory and
        compute costs because no separate critic network is required.
    </p>
    <h3>Simple GRPO Implementation Sketch</h3>
    <p>
        While GRPO was designed for training language models, the underlying idea is simple and
        can be applied to toy problems. Below is a sketch of how one might implement GRPO for the
        pricing environment. This code is illustrative and omits many details needed for a full
        implementation (e.g. reward modelling, sampling many responses, KL penalties).
    </p>
    <pre><code class="language-python"># pseudo-code illustrating GRPO for pricing environment
num_groups = 32  # number of groups per update
group_size = 8   # number of trajectories per group

for update in range(num_updates):
    all_returns = []
    log_probs = []
    # collect grouped trajectories
    for g in range(num_groups):
        group_rewards = []
        group_log_probs = []
        for i in range(group_size):
            # sample trajectory from current policy
            states, actions, rewards, logp = collect_trajectory(policy)
            total_return = sum(rewards)  # episodic reward
            group_rewards.append(total_return)
            group_log_probs.append(sum(logp))
        # normalise rewards within group
        mean_r = np.mean(group_rewards)
        std_r = np.std(group_rewards) + 1e-8
        advantages = [(r - mean_r) / std_r for r in group_rewards]
        all_returns.extend(advantages)
        log_probs.extend(group_log_probs)

    # compute clipped surrogate loss using advantages
    # update policy parameters by gradient ascent
    loss = compute_clipped_loss(log_probs, all_returns)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
</code></pre>
    <p>
        This sketch shows how group‑normalised advantages replace a critic. When applying GRPO to
        domains beyond language, one needs a reward function appropriate for the task. In pricing
        tasks, that reward is simply revenue; in language modelling tasks, it could be a learned
        reward model capturing human preferences.
    </p>
</section>

<section id="optimisation">
    <h2>Optimisation: Gradient Descent and Adam</h2>
    <p>
        Most RL algorithms rely on optimisation to adjust model parameters. Gradient descent
        iteratively updates parameters in the opposite direction of the gradient of a cost function.
        GeeksforGeeks provides the update rule for gradient descent:
    </p>
    <pre><code>w \leftarrow w - \alpha \frac{\partial J(w,b)}{\partial w}, \quad
b \leftarrow b - \alpha \frac{\partial J(w,b)}{\partial b}</code></pre>
    <p>
        where \(\alpha\) is the learning rate【578558331187802†L133-L137】. Gradient descent is widely used
        in linear and logistic regression, support vector machines and neural networks【578558331187802†L82-L104】.
    </p>
    <h3>Toy Problem: Minimising a Quadratic Function</h3>
    <p>
        Consider the function \(f(x) = (x-3)^2\). The minimum is at \(x=3\). We can use gradient
        descent to find this minimum numerically. The gradient of \(f\) with respect to \(x\) is
        \(2(x-3)\). The update rule becomes \(x \leftarrow x - \alpha \cdot 2(x-3)\).
    </p>
    <pre><code class="language-python">import numpy as np

# objective function f(x) = (x - 3)^2 and its gradient
f = lambda x: (x - 3) ** 2
grad = lambda x: 2 * (x - 3)

x = 0.0          # initial guess
alpha = 0.1      # learning rate
for i in range(20):
    x = x - alpha * grad(x)
    print(f"Iter {i:2d}: x={x:.4f}, f(x)={f(x):.6f}")
</code></pre>
    <p>
        The sequence of values for <code>x</code> converges towards 3, demonstrating basic gradient descent.
    </p>
    <h3>Adam Optimisation</h3>
    <p>
        The Adam optimiser (adaptive moment estimation) builds on gradient descent by maintaining
        exponentially decaying averages of past gradients and squared gradients. GeeksforGeeks notes
        that Adam combines the benefits of momentum, Adagrad and RMSprop; it adapts the learning
        rate for each parameter based on both the gradient mean and variance【578558331187802†L659-L670】.
        This makes Adam robust and efficient for training deep networks.
    </p>
    <p>
        Here is a simple implementation of Adam for our quadratic function:
    </p>
    <pre><code class="language-python">import numpy as np

f = lambda x: (x - 3) ** 2
grad = lambda x: 2 * (x - 3)

x = 0.0
m = 0.0  # first moment estimate
v = 0.0  # second moment estimate
beta1 = 0.9
beta2 = 0.999
alpha = 0.1
epsilon = 1e-8

for i in range(1, 51):
    g = grad(x)
    m = beta1 * m + (1 - beta1) * g
    v = beta2 * v + (1 - beta2) * (g ** 2)
    m_hat = m / (1 - beta1 ** i)
    v_hat = v / (1 - beta2 ** i)
    x = x - alpha * m_hat / (np.sqrt(v_hat) + epsilon)
    if i % 10 == 0:
        print(f"Iter {i:2d}: x={x:.6f}, f(x)={f(x):.8f}")
</code></pre>
    <p>
        Adam quickly converges to the minimum with far fewer oscillations than plain gradient descent.
        This is why it is the default optimiser in many deep learning frameworks.
    </p>
</section>

<section id="conclusion">
    <h2>Conclusion</h2>
    <p>
        This guide surveyed the foundations and modern algorithms of reinforcement learning.
        Starting from arithmetic in Python, we defined MDPs and POMDPs, introduced Q‑learning and
        deep Q‑networks for value‑based learning, explored policy‑gradient methods such as PPO,
        discussed the emerging GRPO algorithm for RLHF, and covered optimisation techniques like
        gradient descent and Adam. The toy problems emphasised pricing and decision making rather
        than robotics, illustrating that RL can be applied to a wide range of domains.
    </p>
    <p>
        Reinforcement learning is a vast field. Curious readers should explore value iteration,
        actor‑critic methods, multi‑armed bandits, hierarchical RL and meta‑learning. The
        references cited provide entry points for deeper study of the theory and practice of
        reinforcement learning.
    </p>
</section>

<footer>
    <p><strong>Sources</strong>: [1] MDP definition【320210096800443†L118-L136】. [2] POMDP components【467555468819639†L123-L134】
and belief updates【467555468819639†L140-L151】. [3] Q‑learning algorithm and update rule【961484668398399†L82-L86】【961484668398399†L115-L129】【961484668398399†L131-L140】. [4] PPO description and objectives【724650800826684†L93-L112】【724650800826684†L124-L136】. [5] GRPO group
normalisation【910949881062373†L382-L403】 and surrogate loss【910949881062373†L426-L441】. [6] Gradient descent update rule【578558331187802†L133-L137】
and Adam explanation【578558331187802†L659-L670】. [7] NumPy arithmetic operations【115198363139400†L353-L364】.
    </p>
</footer>
</body>
</html>
