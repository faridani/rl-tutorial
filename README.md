# rl-tutorial
Tutorials for Learning Reinforcement Learning for Smart People 

Download the course material first and then use the following links to go through the material [download](https://github.com/faridani/rl-tutorial/archive/refs/heads/main.zip) 


# Part 0 - Mathmetics for RL
The following two lessons should prepare you for almost all of the math that you will see in RL 
* Chapter 1: College level mathematics [quick tutorial](part-0/basic_math.html)
* Chapter 2: 1st year grad level mathematics [quick tutorial](part-0/more_math.html)

# Part I — Foundations

## Chapter 1. An RL Primer [visit lesson](part-1/chapter1.html) 
* Agent–Environment Interface
* Rewards, Returns, and Discounting
* Episodic vs. Continuing Tasks
* Exploration–Exploitation

## Chapter 2. Mathematical Toolkit [visit lesson](part-1/chapter2.html) 
* Probability & Expectation
* Markov Chains & Stationarity
* Bellman Equations & Contractions
* Linear Algebra Essentials

## Chaper 3. Optimization Foundations for RL [visit lesson](part-1/chapter3.html) 
* Gradient Descent & Stochastic Approximation
* Momentum, RMSProp & Adam
* Natural Gradient & Fisher Information
* Constrained Optimization & Lagrangians

## Chapter 4. Python & Tools for RL [visit lesson](part-1/chapter4.html) 
* NumPy & Vectorization
* Environment APIs (Gymnasium, PettingZoo)
* Experiment Tracking & Reproducibility
* Testing & Debugging RL

# Part II — Bandits & MDPs 

## Chapter 5. Multi‑Armed Bandits [visit lesson](part-2/chapter5.html) 
* ε‑Greedy Strategies
* Upper Confidence Bounds (UCB)
* Thompson Sampling
* Contextual Bandits & LinUCB

## Chapter 6. Markov Decision Processes [visit lesson](part-2/chapter6.html) 

* MDP Formalism
* Value Functions & Bellman Operators
* Policy Evaluation
* Policy & Value Iteration

## Chapter 7. Partially Observable MDPs [visit lesson](part-2/chapter7.html) 

* Belief States & Bayes Filters
* Point‑Based Planning
* POMDPs with Recurrent Policies
* Information Gathering

# Part III — Value Learning & Control

## Chapter 8. Monte Carlo & Temporal‑Difference Learning [visit lesson](part-3/chapter8.html) 

* First‑Visit & Every‑Visit MC
* TD(0) & n‑Step TD
* Eligibility Traces & TD(λ)
* Bias–Variance Tradeoffs

## Chapter 9. Tabular Control Methods [visit lesson](part-3/chapter9.html) 

* SARSA
* Q‑Learning
* Expected SARSA
* Double Learning

## Chapter 10. Exploration in Control [visit lesson](part-3/chapter10.html) 

* Epsilon Schedules & Softmax
* Count‑Based Exploration
* Intrinsic Motivation (RND, ICM)
* Directed Information Gain

## Chapter 11. Function Approximation [visit lesson](part-3/chapter11.html) 

* Linear Approximation
* Feature Construction
* Gradient TD Methods
* Regularization & Stability

# Part IV — Deep RL

## Chapter 12. Deep Q‑Learning [visit lesson](part-4/chapter12.html) 
* DQN Architecture
* Experience Replay & Target Networks
* Stability Pathologies
* Data‑Efficiency Tricks

## Chapter 13. Advanced Value Methods  [visit lesson](part-4/chapter13.html) 
* Double & Dueling DQN
* Prioritized Replay
* Distributional RL (C51, QR‑DQN)
* Noisy Nets & Parameter Noise

## Chapter 14. Policy Gradient Methods  [visit lesson](part-4/chapter14.html) 
* REINFORCE & Baselines
* Generalized Advantage Estimation (GAE)
* Natural Gradients
* Entropy Regularization

## Chapter 15. Actor–Critic & Trust Regions  [visit lesson](part-4/chapter15.html) 
* A2C/A3C
* TRPO
* PPO (Clip & Penalty)
* Stability & Hyperparameters

## Chapter 16. Maximum Entropy & Continuous Control  [visit lesson](part-4/chapter16.html) 
* Soft Actor–Critic (SAC)
* Twin Delayed DDPG (TD3)
* Deterministic Policies (DDPG)
* Action Squashing & Exploration Noise

## Chapter 17. Model‑Based RL  [visit lesson](part-4/chapter17.html) 
* Dyna‑Style Planning
* Latent Dynamics Models
* Model‑Predictive Control (MPC)
* Uncertainty & Ensembles


# Part V — Structure & Constraints

## Chapter 18. Hierarchical & Goal‑Conditioned RL [visit lesson](part-5/chapter18.html) 
* Options & Skills
* Subgoal Discovery
* Goal‑Conditioned Policies
* Hindsight Experience Replay (HER)

## Chapter 19. Offline & Batch RL  [visit lesson](part-5/chapter19.html) 
* Off‑Policy Evaluation (IS, WIS, DR)
* Batch‑Constrained Q‑Learning (BCQ)
* BEAR & CQL
* Distribution Shift & Conservatism

## Chapter 20. Safe & Risk‑Sensitive RL  [visit lesson](part-5/chapter20.html) 
* Constrained MDPs
* Risk Measures (CVaR)
* Robust MDPs
* Shielding & Safety‑Critical Evaluation

## Chapter 21. Multi‑Agent RL  [visit lesson](part-5/chapter21.html) 
* Markov Games
* Centralized Training, Decentralized Execution
* Cooperation vs. Competition
* Opponent Modeling & Equilibria

# Part 4  

* Q-Learning, SARSA, Double Q-Learning Lab [lab and tutorial](q-learning-lab.html)
* Online tutorials worthy of your time [other-resources.html](other-resources.html)
