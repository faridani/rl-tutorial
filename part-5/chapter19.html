<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Chapter 19 – Offline &amp; Batch Reinforcement Learning</title>
  <style>
    body { background-color: #ffffff; font-family: "Georgia", serif; margin: 40px; line-height: 1.6; color: #333333; }
    pre { background: #f4f4f4; padding: 10px; border-left: 4px solid #cccccc; overflow-x: auto; }
    code { font-family: monospace; }
    h1, h2, h3 { color: #222222; }
  </style>
<script>
  MathJax = {
    tex: {
      inlineMath: [['\\(', '\\)']],
      displayMath: [['\\[', '\\]']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

  
</head>
<body>
  <h1>Chapter 19 – Offline &amp; Batch Reinforcement Learning</h1>
  <p>
    Traditional reinforcement learning algorithms rely on continuous interaction with the
    environment to collect data, adjust policies and evaluate performance.  In many domains
    interaction is expensive, risky or impossible.  Offline (or batch) reinforcement learning aims
    to learn effective policies from a fixed dataset of past experiences, without any further
    environment interaction.  This chapter introduces the principles of offline RL, explains how
    to evaluate policies off‑line, discusses conservative algorithms designed to mitigate
    distribution shift and highlights the importance of data quality and benchmarks.
  </p>

  <h2>19.1 Definition and motivation</h2>
  <p>
    Offline reinforcement learning uses a static dataset of state–action–reward–next state tuples to
    train a policy or value function.  Unlike online RL, the agent does not explore the environment
    during training.  As the Milvus reference explains, the agent learns a policy “using a fixed
    dataset of past experiences, without interacting with the environment during training”【924020056287568†L72-L76】.
    The data may come from historical logs, human demonstrations or simulations.  Offline RL is
    particularly useful when deploying new policies poses safety risks or high costs, such as in
    healthcare, finance or industrial control【924020056287568†L84-L89】.
  </p>
  <p>
    The key distinction from online learning introduces unique challenges.  Because the policy
    cannot collect new data, it must contend with <em>distributional shift</em> between the behaviour
    policy that generated the data and the evaluation policy.  Actions chosen by the learned policy
    may fall outside the support of the dataset, leading to unpredictable outcomes【924020056287568†L90-L96】.
    To address this, offline algorithms often constrain the policy to remain close to the behaviour
    policy or penalise actions not observed in the data.
  </p>
  <p>
    Offline RL is distinct from imitation learning, which seeks to mimic expert behaviour
    regardless of returns.  Instead, offline RL aims to optimise an objective (e.g., long‑term
    profit) based on historical data, which may include suboptimal actions.  Examples include
    learning dynamic pricing strategies from e‑commerce transaction logs or optimising treatment
    plans from medical records.  The potential impact of offline RL is enormous, but it requires
    careful attention to evaluation and robustness.
  </p>

  <h2>19.2 Off‑policy evaluation and importance sampling</h2>
  <p>
    Before deploying a policy learned from offline data, one must estimate its performance without
    executing it.  Off‑policy evaluation (OPE) seeks to estimate the expected return of an
    evaluation policy using data generated by a different behaviour policy.  One simple OPE method
    is <em>importance sampling</em>.  Given trajectories sampled under the behaviour policy
    \(\mu\), we compute the ratio of the evaluation policy \(\pi\) to the behaviour policy at each
    time step and weight returns accordingly: \(\hat{J} = \frac{1}{N} \sum_{i=1}^N W_i G_i\), where
    \(W_i = \prod_{t} \frac{\pi(a_t^i|s_t^i)}{\mu(a_t^i|s_t^i)}\) and \(G_i\) is the return of the
    i‑th trajectory.  Importance sampling yields an unbiased estimate but suffers from high
    variance, especially when the policies differ significantly.
  </p>
  <p>
    Weighted importance sampling normalises the weights to sum to one, reducing variance at the
    cost of bias.  Other OPE methods include doubly robust estimators and model‑based approaches
    that learn an approximate model of the environment.  In pricing applications, OPE allows
    practitioners to estimate the profitability of new pricing strategies using past sales data
    before deploying them, thereby avoiding costly experiments.
  </p>
  <p>
    The following code demonstrates a basic importance sampling estimator for episodic returns.  It
    takes a dataset of trajectories along with behaviour and evaluation policies and computes the
    weighted returns.  Although simplistic, this illustrates the mechanics of OPE and can be
    extended to weighted or doubly robust estimators.
  </p>
  <pre><code class="language-python">def importance_sampling(trajectories, eval_policy, behav_policy, gamma=1.0):
    """Estimate expected return of eval_policy using trajectories from behav_policy."""
    returns = []
    for traj in trajectories:
        W = 1.0
        G = 0.0
        for t, (s, a, r) in enumerate(traj):
            # accumulate discounted reward
            G += (gamma ** t) * r
            # update weight ratio
            W *= eval_policy[s][a] / behav_policy[s][a]
        returns.append(W * G)
    return sum(returns) / len(returns)

# Example policies and trajectories
behav_policy = {0: {0: 0.8, 1: 0.2}, 1: {0: 0.5, 1: 0.5}}
eval_policy = {0: {0: 0.5, 1: 0.5}, 1: {0: 0.2, 1: 0.8}}
trajectories = [
    [(0,0,1),(1,1,2)],
    [(0,1,0),(1,0,3)],
]
estimate = importance_sampling(trajectories, eval_policy, behav_policy)
print("Off-policy estimate:", estimate)
  </code></pre>

  <h2>19.3 Conservative algorithms: BCQ, BEAR and CQL</h2>
  <p>
    Offline learning is vulnerable to <em>extrapolation error</em>: Q‑values can become inaccurate when
    evaluated at state–action pairs not represented in the dataset.  To mitigate this, conservative
    algorithms restrict the policy to remain within the support of the data.  Batch-Constrained
    Q‑learning (BCQ) uses a generative model of the behaviour policy to sample candidate actions
    and applies a perturbation model to adjust them.  Actions far from the dataset have low
    probability under the generative model and are thus unlikely to be selected.  Conservative Q
    Learning (CQL) takes a different approach: it penalises Q‑values of unseen actions so that
    unseen actions are assigned lower values【924020056287568†L90-L96】.  Both methods aim to prevent
    the learned policy from taking actions for which there is insufficient evidence in the data.
  </p>
  <p>
    Another algorithm, Bootstrapping Error Accumulation Reduction (BEAR), minimises the
    distributional shift between the learned policy and the behaviour policy using a maximum
    mean discrepancy (MMD) constraint.  BEAR encourages the policy’s action distribution to be
    close to that of the behaviour policy while still improving performance.  These conservative
    methods have been shown to outperform naive offline Q‑learning, particularly on benchmark
    datasets such as D4RL.
  </p>
  <p>
    The following snippet outlines a simplified offline Q‑learning algorithm with action
    constraints.  The action selection uses a weighted sample from the behaviour policy (to mimic
    BCQ) and updates the Q function only at observed state–action pairs.  While rudimentary, this
    example illustrates how offline RL incorporates behaviour policy information during training.
  </p>
  <pre><code class="language-python">import random

def offline_q_learning(dataset, behav_policy, n_states, n_actions, alpha=0.1, gamma=0.99, epochs=10):
    Q = [[0.0 for _ in range(n_actions)] for _ in range(n_states)]
    for _ in range(epochs):
        for (s,a,r,s_next) in dataset:
            # sample candidate action from behaviour policy for next state
            a_next_candidates = list(behav_policy[s_next].keys())
            probs = list(behav_policy[s_next].values())
            a_next = random.choices(a_next_candidates, probs)[0]
            target = r + gamma * Q[s_next][a_next]
            Q[s][a] += alpha * (target - Q[s][a])
    return Q

# Example dataset of transitions and behaviour policy
dataset = [(0,0,1,1), (1,1,2,0), (0,1,0,1)]
Q = offline_q_learning(dataset, behav_policy, n_states=2, n_actions=2)
print(Q)
  </code></pre>

  <h2>19.4 Data considerations and benchmarks</h2>
  <p>
    The performance of offline RL algorithms depends heavily on the quality and coverage of the
    dataset.  If the dataset contains biased or narrow behaviour, the learned policy may inherit
    these biases and perform poorly.  The Milvus article emphasises that “developers applying
    offline RL must prioritise data quality and coverage”【924020056287568†L98-L109】.  For
    recommendation systems, using historical interaction logs that cover a diverse set of user
    behaviours helps the learned policy generalise.  In pricing, datasets should include a range
    of price points and market conditions.
  </p>
  <p>
    To evaluate offline RL methods, researchers have developed benchmark suites.  The D4RL
    benchmark provides datasets from tasks such as locomotion, navigation and manipulation with
    varying difficulty and behaviour quality.  By comparing algorithms on common benchmarks,
    practitioners can better understand their strengths and weaknesses.  When using offline RL in
    practice, one should also validate policies in simulation or limited online tests before
    full deployment to ensure safety and robustness.
  </p>
  <p>
    In summary, offline RL opens new opportunities for data‑driven decision making without
    environment interaction.  However, it requires careful algorithm design to avoid extrapolation
    errors and an emphasis on high‑quality datasets to ensure reliable performance.
  </p>

  <h2>Sources</h2>
  <p>
    The definition of offline RL and its benefits and challenges are drawn from Milvus’ overview of
    offline RL, which explains that offline RL learns from a fixed dataset without real‑time
    interaction【924020056287568†L72-L76】, highlights the challenge of distributional shift and
    mentions conservative algorithms like BCQ and CQL【924020056287568†L90-L96】.  The importance of data
    quality and coverage is emphasised in the same source【924020056287568†L98-L109】.  These citations
    provide the context for the methods discussed in this chapter.
  </p>
</body>
</html>
