<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Chapter 6 â€“ Markov Decision Processes</title>
<style>
  body {
    background-color: #ffffff;
    font-family: 'Georgia', 'Times New Roman', serif;
    margin: 40px;
    line-height: 1.6;
    color: #333333;
  }
  h1 {
    color: #2c3e50;
    font-size: 2.4em;
    margin-bottom: 0.2em;
  }
  h2 {
    color: #34495e;
    font-size: 1.8em;
    margin-top: 1.5em;
    margin-bottom: 0.5em;
  }
  p {
    font-size: 1.05em;
    margin-bottom: 1em;
    text-align: justify;
  }
  pre {
    background-color: #f7f7f7;
    border: 1px solid #e1e1e8;
    padding: 16px;
    overflow-x: auto;
    border-radius: 6px;
  }
  ul {
    margin-left: 1.2em;
  }
</style>
</head>
<body>
<h1>Chapter 6 â€“ Markov Decision Processes</h1>
<p>The Markov decision process (MDP) is the canonical model for sequential decision making under uncertainty.  In reinforcement learning it plays the same foundational role that probability distributions play in statistics: it specifies the interaction between an <em>agent</em> and its <em>environment</em> over time.  The agent moves through a set of states, selects actions, receives rewards and transitions to new states according to a stochastic transition model.  In this chapter we formalise MDPs, introduce the value functions and Bellman operators that underlie dynamic programming, and show how to compute optimal policies via policy and value iteration.  Throughout we use small pricing and decisionâ€‘making examples rather than robotics to illustrate the ideas.</p>

<h2>6.1Â MDP formalism</h2>
<p>An MDP is defined by a finite or countable set of states S, a set of actions A, a transition probability distribution P<sub>a</sub>(s,sâ€²) describing the probability of moving from state s to state sâ€² when action a is executed, and an immediate reward function R<sub>a</sub>(s,sâ€²) giving the payoff for that transition.  In the canonical discounted formulation the quadruple (S, A, P<sub>a</sub>, R<sub>a</sub>) defines a stochastic control process with Markov dynamics: the next state depends only on the current state and actionã€567934665676015â€ L192-L218ã€‘.  The agent also chooses a policy Ï€, a mapping from states to actions that can be deterministic or stochastic, and we often write a<sub>t</sub>Â =Â Ï€(s<sub>t</sub>).</p>
<p>The objective in an MDP is to maximise the expected cumulative reward over time.  When rewards may extend into the future one discounts later rewards by a factor Î³Â âˆˆÂ [0,1].  The expected discounted return for a policy Ï€ starting in state s is
\[V^Ï€(s)Â =Â E[âˆ‘<sub>t=0</sub>^âˆÂ Î³^tÂ R<sub>a_t</sub>(s_t,s_{t+1})Â |Â sâ‚€Â =Â s,Â a_tÂ =Â Ï€(s_t)],\]
where the expectation is taken over trajectories generated by the transition dynamics.  Choosing a good policy therefore involves balancing immediate rewards against future gains.  A policy that maximises the above objective for all states is called an optimal policyã€567934665676015â€ L236-L272ã€‘.</p>
<p>An equivalent but sometimes more intuitive formulation uses a fiveâ€‘tuple (S,Â A,Â T,Â R,Â Î³) where T(s,a,sâ€²) denotes the probability of transitioning to sâ€² from s under action a and R(s,a,sâ€²) is the immediate reward.  In practice one often works with a simulator or generative model that samples (sâ€²,r)Â âˆ¼Â G(s,a) rather than storing the full transition matrixã€567934665676015â€ L275-L297ã€‘.  In all cases the Markov property implies that the probability of the next state depends only on the current state and action, not on previous history.</p>
<p>To give a concrete example imagine a retailer choosing the price of a product each day.  The state s might encode the current day and remaining inventory, the action a is the price to charge, the transition probabilities capture stochastic demand causing inventory to decrease, and the reward is the profit obtained.  The retailerâ€™s goal is to choose prices to maximise the discounted sum of expected profits.  We will revisit pricing examples throughout this chapter.</p>

<h2>6.2Â Value functions and Bellman operators</h2>
<p>To reason about an MDP one defines value functions that quantify the longâ€‘term quality of states and actions.  The stateâ€‘value function V^Ï€(s) for a policy Ï€ is the expected discounted return starting from state s and following Ï€.  Similarly the actionâ€‘value function Q^Ï€(s,a) is the expected return starting from state s, taking action a initially and following Ï€ thereafter:
\[Q^Ï€(s,a)Â =Â E[Â R<sub>a</sub>(s,sâ‚)Â +Â Î³Â V^Ï€(sâ‚)Â ],\]
where sâ‚ is drawn from the transition distribution for action a.  These functions satisfy a family of recursive relationships known as the Bellman expectation equations.  For the stateâ€‘value function the equation reads
\[V^Ï€(s)Â =Â R(s,Ï€(s))Â +Â Î³Â âˆ‘<sub>sâ€²</sub>Â P_{Ï€(s)}(s,sâ€²)Â V^Ï€(sâ€²),\]
and for the actionâ€‘value function
\[Q^Ï€(s,a)Â =Â R(s,a)Â +Â Î³Â âˆ‘<sub>sâ€²</sub>Â P_a(s,sâ€²)Â Q^Ï€(sâ€²,Ï€(sâ€²)).\]
</p>
<p>These equations imply that the value functions are fixed points of a mapping called the Bellman operator.  Denote by ğ”…^Ï€ the operator that maps a function Q to ğ”…^Ï€[Q] defined by the rightâ€‘hand side of the Bellman equation.  One can show that ğ”…^Ï€ is a contraction mapping: for any two functions Q and U we have â€–ğ”…^Ï€[Q]Â âˆ’Â ğ”…^Ï€[U]â€–<sub>âˆ</sub>Â â‰¤Â Î³Â â€–QÂ âˆ’Â Uâ€–<sub>âˆ</sub>.  Consequently repeated application of the Bellman operator starting from an arbitrary function converges to the unique fixed point Q^Ï€ã€221921659924593â€ L21-L63ã€‘.</p>
<p>The contraction property not only guarantees convergence but also provides a way to quantify the rate of convergence: the error decreases geometrically by a factor of Î³ at each iteration.  In practice the Bellman operator is the workhorse behind dynamic programming algorithms such as policy evaluation, policy iteration and value iteration.  Understanding the value functions and the Bellman operator is therefore fundamental before we can discuss algorithms for computing optimal policies.</p>

<h2>6.3Â Policy evaluation</h2>
<p>Policy evaluation is the task of computing the value function V^Ï€ for a given policy Ï€.  Because the value function appears on both sides of the Bellman expectation equation, one typically solves it via iterative methods.  Starting with an initial estimate Vâ‚€(s)Â =Â 0 for all states, one repeatedly applies the Bellman expectation update
\[V_{k+1}(s)Â â†Â R(s,Ï€(s))Â +Â Î³Â âˆ‘<sub>sâ€²</sub>Â P_{Ï€(s)}(s,sâ€²)Â V_k(sâ€²)\]
until the changes between successive iterates become negligible.  This procedure is guaranteed to converge to the true value function because the underlying Bellman operator is a contractionã€221921659924593â€ L21-L63ã€‘.  The same technique can be applied to action values Q^Ï€.</p>
<p>The following Python function implements policy evaluation for a small pricing MDP.  The state space consists of inventory levels 0,1,â€¦,m; the action space contains two prices: a high price and a low price.  Demand at the low price is higher on average but yields less profit per unit.  We assume the policy always chooses the high price when inventory is low and the low price when inventory is high.  The function returns the estimated value of each inventory level.</p>
<pre><code class="language-python">import numpy as np

# Transition and reward models for a simple pricing MDP
# states: inventory levels 0..m, actions: 0=low price, 1=high price
m = 3
P = np.zeros((2, m+1, m+1))
R = np.zeros((2, m+1, m+1))
# populate transition and reward matrices
for s in range(m+1):
    for a in [0, 1]:
        # demand probabilities: low price sells 0,1,2 units; high price sells 0 or 1 unit
        if a == 0:
            probs = [0.2, 0.5, 0.3]  # low price demand distribution
            reward_per_unit = 5
        else:
            probs = [0.5, 0.5]      # high price demand distribution
            reward_per_unit = 8
        for units_sold, prob in enumerate(probs):
            next_s = max(0, s - units_sold)
            P[a, s, next_s] += prob
            R[a, s, next_s] += prob * units_sold * reward_per_unit

# fixed policy: if inventory â‰¥ 2 choose low price (0), else choose high price (1)
policy = np.array([1 if s < 2 else 0 for s in range(m+1)])

def policy_evaluation(policy, P, R, gamma=0.9, tol=1e-4):
    V = np.zeros(P.shape[1])
    while True:
        delta = 0
        for s in range(P.shape[1]):
            a = policy[s]
            v_new = (R[a, s] + gamma * P[a, s] @ V).sum()
            delta = max(delta, abs(v_new - V[s]))
            V[s] = v_new
        if delta < tol:
            break
    return V

values = policy_evaluation(policy, P, R)
print("Inventory values under the fixed policy:", values)
</code></pre>
<p>Running this code prints the longâ€‘term value of each inventory level when following the specified pricing policy.  More sophisticated environments with richer state and action spaces are solved in the same way; the key is to compute expected rewards and discounted values under the transition dynamics and policy.</p>

<h2>6.4Â Policy and value iteration</h2>
<p>Computing an optimal policy requires more than evaluating a fixed policy; one must improve the policy based on the value estimates.  The two classical dynamic programming algorithms are <em>value iteration</em> and <em>policy iteration</em>.  Value iteration directly seeks the optimal value function V* by repeatedly applying the Bellman optimality update
\[V_{k+1}(s)Â â†Â max<sub>a</sub>Â [Â R(s,a)Â +Â Î³Â âˆ‘<sub>sâ€²</sub>Â P_a(s,sâ€²)Â V_k(sâ€²)Â ],\]
which follows from the Bellman optimality equationã€862037459657812â€ L87-L110ã€‘.  Once the value function converges, the optimal policy is obtained by choosing for each state the action that attains the maximum on the rightâ€‘hand side.  Value iteration is simple to implement but performs a full maximisation over actions at every state at each iteration.</p>
<p>Policy iteration, by contrast, alternates two phases: policy evaluation (as described above) and <em>policy improvement</em>.  For a current policy Ï€ one first computes its value function V^Ï€.  Then one improves the policy by greedily selecting at each state the action that maximises the expected return according to V^Ï€:
\[Ï€â€²(s)Â =Â argmax<sub>a</sub>Â [Â R(s,a)Â +Â Î³Â âˆ‘<sub>sâ€²</sub>Â P_a(s,sâ€²)Â V^Ï€(sâ€²)Â ].\]
Policy iteration repeats evaluation and improvement until the policy no longer changes; it can converge more quickly than value iteration because it avoids repeatedly reâ€‘maximising the value functionã€862037459657812â€ L112-L133ã€‘.  In practice one may use a hybrid method, modified policy iteration, that performs a few evaluation steps between improvements.</p>
<p>The following code implements value iteration for the pricing MDP defined earlier.  It computes the optimal price to charge at each inventory level.  After convergence, the resulting policy recommends the high or low price depending on whether the marginal value of inventory outweighs the immediate reward.</p>
<pre><code class="language-python">def value_iteration(P, R, gamma=0.9, tol=1e-4):
    num_states = P.shape[1]
    num_actions = P.shape[0]
    V = np.zeros(num_states)
    while True:
        delta = 0
        for s in range(num_states):
            v_old = V[s]
            # compute value for all actions
            q_sa = np.zeros(num_actions)
            for a in range(num_actions):
                q_sa[a] = (R[a, s] + gamma * P[a, s] @ V).sum()
            V[s] = np.max(q_sa)
            delta = max(delta, abs(v_old - V[s]))
        if delta < tol:
            break
    # derive policy
    policy = np.zeros(num_states, dtype=int)
    for s in range(num_states):
        q_sa = [(R[a, s] + gamma * P[a, s] @ V).sum() for a in range(num_actions)]
        policy[s] = int(np.argmax(q_sa))
    return V, policy

V_opt, policy_opt = value_iteration(P, R)
print("Optimal inventory values:", V_opt)
print("Optimal pricing policy (0=low, 1=high):", policy_opt)
</code></pre>
<p>When applied to our small inventory example the optimal policy typically prices high when the inventory is low to maximise immediate profit and prices low when inventory is plentiful to encourage sales.  The dynamic programming algorithms described above thus allow us to compute optimal policies in tabular settings.  In larger state spaces one turns to approximate dynamic programming and reinforcement learning algorithms discussed in subsequent chapters.</p>

<h2>Sources</h2>
<ul>
<li>Definition of an MDP as a 4â€‘tuple of states, actions, transition probabilities and rewards and the objective to maximise expected discounted returnsã€567934665676015â€ L192-L252ã€‘.</li>
<li>Contraction property of the Bellman operator and convergence to the unique value functionã€221921659924593â€ L21-L63ã€‘.</li>
<li>Bellman optimality equation and definitions of value and policy iterationã€862037459657812â€ L87-L133ã€‘.</li>
</ul>
</body>
</html>
