
# Part VI — Imitation, Preferences & LLMs
## Chapter 22. Imitation & Inverse RL [visit lesson](chapter22.html) 
* Behavior Cloning
* DAgger & Dataset Aggregation
* Maximum‑Entropy IRL
* Adversarial Imitation (GAIL, AIRL)

## Chapter 23. Preference‑Based RL & Alignment [visit lesson](chapter23.html) 
* Reward Modeling from Human Feedback
* RLHF with PPO
* GRPO (Group Relative Policy Optimization)
* Safety, Bias & Evaluation
